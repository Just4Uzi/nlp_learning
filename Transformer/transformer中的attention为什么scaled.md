来源: https://www.zhihu.com/question/339723385

论文中解释是：向量的点积结果会很大，将softmax函数push到梯度很小的区域，scaled会缓解这种现象。
怎么理解将sotfmax函数push到梯度很小区域？还有为什么scaled是维度的根号，不是其他的数？

1. 为什么比较大的输入会使得softmax的梯度变得很小？

对于一个输入向量 X ∈ R^d，softmax函数将其映射到一个归一化分布 y∈R^d。
在这个过程中，softmax先用一个自然底数e将输入向量元素之间的差距拉大，然后归一化为一个分布。假设某个输入X中最大的元素下标是k，
则如果输入的数量级变大（即每个元素都很大），那么yk会非常接近于1。

数量级对softmax得到的分布影响非常大。在数量级较大时，softmax将几乎**全部**的概率分布都分配给了最大值对应的标签。

然后我们来看softmax的梯度，不妨简记softmax函数为g(), softmax函数得到的分布向量y=g(x)对输入x的梯度为diag(y) - y*y^T，即y的对角矩阵减去y矩阵乘y矩阵的转置。

而假设x1为最大，y接近于[1, 0, 0, ..., 0] ^T, 此时计算出梯度消失为0，造成参数更新困难。


2. self-attention中维度与点积大小的关系是怎么样的，为什么使用维度的根号为放缩？

假设向量q和k的均值为0，方差为1，那么点积q * k的均值为0，方差为是dk，其中dk是向量q和k的维度。

证明如下：

对于任意i∈1,...,dk, qi和ki都是随机变量，不妨记X=qi，Y=ki，那么有D(X) = D(Y) = 1，E(X) = E(Y) = 0

则有

    * E(XY) = E(X)E(Y) = 0
    * D(XY) = E(X^2 * Y^2) - [E(XY)]^2
            = E(X^2)E(Y^2) - [E(X) * E(Y)]^2
            = E(X^2 - 0)E(Y^2 - 0) - [E(X)E(Y)]^2
            = E(X^2 - [E(X)]^2)E(Y^2 - [E(Y)]^2) - [E(X)E(Y)]^2
            = D(X)D(Y) - [E(X)E(Y)]^2
            = 1 * 1 - 0 * 0
            = 1
那么对于任意i∈1,...,dk, qi*ki的均值为0，方差为1，对于相互独立的分量Zi，有
E(∑Zi) = ∑E(Zi)
D(∑Zi) = ∑D(Zi)
所以q*k的均值E(q*k)=0，方差D(q*k) = dk。
方差越大说明，点积的数量级越大（以越大的概率取大值）。那么一个自然的做法就是把方差稳定到1，做法就是点积除以√dk，这样有
D(q*k/√dk) = D(q*k) / (√dk)^2 = dk / dk = 1
将方差控制为1，也就有效控制了前面提到的梯度消失为题。


3. 为什么普通的attention不使用scaled？

    * 基础的attention有两种，一个Add，一个Mul, <>表示矩阵的点积
        Score(h,s) = <v, tanh(W1h + W2s)>
        Score(h,s) = <W1h, W2s>
    * 关于两种计算方法的效率，Add方法虽然计算简单，但是外面套着tanh和v，相当于一个完整的隐层；而矩阵乘法已经有了非常成熟的加速实现。
      在dk（h和s的向量维度）较小时，两者的效果接近，但是随着dk增大，Add效率会明显提高

    * Mul使用scaled的原因，是因为极大的点积值将softmax推向梯度平缓区，使得收敛非常困难，才有了scaled。Add中只有随机变量X与参数矩阵W相乘，而后者包含随机变量X与随机变量X相乘。
    * 如果s和h都分布在[0,1]，那么相乘时一次对所有位置的∑求和，整体的分布就会扩大到[0, dk]；而Add，右侧是tanh，分布在[-1, 1]，整体分布与dk没有关系。

4. 为什么分类层（最后一层），使用非scaled的softmax？

    * 这一层的softmax也没有两个随机变量相乘的情况，都是随机变量X与参数矩阵W相乘。
    * 这一层的softmax通常和交叉熵联合求导，在某个目标类别上的整体梯度为yi` - yi，即预测值与真实值的差值。
      当出现某个极大的元素值，softmax的输出概率会集中在改类别上。如果预测正确，整体梯度接近于0，抑制参数更新；如果是错误类别，则整体梯度为1，给出最大程度的负反馈。

    * 也就是说，这个时候的梯度形式改变，不会出现极大值导致梯度消失的情况了。


